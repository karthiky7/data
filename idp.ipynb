{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsglue.context import GlueContext\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import DataFrame\n",
    "import boto3\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from awsglue.job import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job_bookmark_option': 'job-bookmark-disable', 'job_bookmark_from': None, 'job_bookmark_to': None, 'JOB_ID': None, 'JOB_RUN_ID': None, 'SECURITY_CONFIGURATION': 'P2pLogisticsGlueSecurityConfiguration', 'encryption_type': None, 'RedshiftTempDir': None, 'TempDir': None, 'JOB_NAME': 'glue-script', 'configbucket': 'data-analytics-config-dev', 'configfile': 'idp-dashboard/etl/conf/landing_to_staging.json', 'landingbucket': 'gbs-idp-dashboard-landing-dev', 'stagingbucket': 'gbs-idp-dashboard-staging-dev', 'format': 'parquet'}\n"
     ]
    }
   ],
   "source": [
    "sys.argv +=[\"--JOB_NAME\",\"glue-script\",\"--landingbucket\",\"gbs-idp-dashboard-landing-dev\",\"--SECURITY_CONFIGURATION\",\"P2pLogisticsGlueSecurityConfiguration\",\"--stagingbucket\",\"gbs-idp-dashboard-staging-dev\",'--configfile',\"idp-dashboard/etl/conf/landing_to_staging.json\",\"--configbucket\",\"data-analytics-config-dev\",\"--format\",\"parquet\"]\n",
    "args = getResolvedOptions(\n",
    "        sys.argv, [ \"JOB_NAME\",\"configbucket\",\"configfile\", \"landingbucket\", \"stagingbucket\",\"format\"]\n",
    "    )\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'JOB_NAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f770910a8e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mglueContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.local.dir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"C:\\\\rpa-logs\\\\\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglueContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'JOB_NAME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglueContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'JOB_NAME'"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "glueContext._sc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "glueContext._sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",os.environ['AWS_ACCESS_KEY_ID'])\n",
    "glueContext._sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",os.environ['AWS_SECRET_ACCESS_KEY'] )\n",
    "glueContext._sc._jsc.hadoopConfiguration().set(\"fs.s3a.session.token\",os.environ['AWS_SESSION_TOKEN'])\n",
    "glueContext.spark_session.conf.set(\"spark.local.dir\", \"C:\\\\rpa-logs\\\\\")\n",
    "job = Job(glueContext)\n",
    "spark = glueContext.spark_session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_header(name):\n",
    "    name = re.sub(r'([A-Z](?=[a-z]))', r'_\\1', name)\n",
    "    name = re.sub(r'((?<=[a-z0-9])[A-Z])', r'_\\1', name)\n",
    "    name = name.lower()\n",
    "    name = re.sub(r'[^\\w]', '_', name)\n",
    "    name = re.sub(r'^(_+)', '', name)\n",
    "    name = re.sub(r'(_+)$', '', name)\n",
    "    name = re.sub(r'(_{2,})', '_', name)\n",
    "    return name\n",
    "def typecasting(df: DataFrame,castinfo) -> DataFrame:\n",
    "        for header in df.columns:\n",
    "            if header in castinfo:\n",
    "                df = df.withColumn(header, df[header].cast(castinfo[header]))\n",
    "        return df\n",
    "\n",
    "def csvtoparquet(spark,sourceinfo,args):\n",
    "    print(sourceinfo)\n",
    "    inputDF=spark.read.option(\"delimiter\",sourceinfo['formatOptions']['sep']).option(\"nullValue\", sourceinfo['formatOptions']['nullValue']).option(\"escape\", sourceinfo['formatOptions']['escape']).option(\"quote\", sourceinfo['formatOptions']['quote']).option(\"header\",sourceinfo['formatOptions']['header']) \\\n",
    "                                   .option(\"inferSchema\",sourceinfo['formatOptions']['inferschema']).csv(sourceinfo['landing_prefix'].replace('{landingbucket}',args['landingbucket']))\n",
    "    inputDF=typecasting(inputDF,sourceinfo['cast'])\n",
    "    for column in inputDF.columns:\n",
    "        inputDF = inputDF.withColumnRenamed(column, format_header(column))\n",
    "    inputDF.repartition(1).write.mode(\"overwrite\").format(args['format']).save(sourceinfo['staging_prefix'].replace('{stagingbucket}',args['stagingbucket']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'landing_prefix': 's3://{landingbucket}/malaysiaBefore', 'staging_prefix': 's3://{stagingbucket}/malaysia-before', 'cast': {'VendorId': 'int', 'PostalCode': 'int', 'VendorCode': 'int'}, 'formatOptions': {'header': 'true', 'sep': ',', 'quote': '\"', 'escape': '\"', 'inferschema': 'true', 'nullValue': 'NULL'}, 'partition_count': 1}\n",
      "{'landing_prefix': 's3://{landingbucket}/malaysiaAfter', 'staging_prefix': 's3://{stagingbucket}/malaysia-after', 'cast': {'VendorId': 'int', 'PostalCode': 'int', 'VendorCode': 'int'}, 'formatOptions': {'header': 'true', 'sep': ',', 'quote': '\"', 'escape': '\"', 'inferschema': 'true', 'nullValue': 'NULL'}, 'partition_count': 1}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = getResolvedOptions(\n",
    "        sys.argv, [ 'JOB_NAME',\"configbucket\",\"configfile\", \"landingbucket\", \"stagingbucket\",\"format\"]\n",
    "    )\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    config_s3_object = s3.Object(args[\"configbucket\"], args[\"configfile\"])\n",
    "    config = json.loads(config_s3_object.get()[\"Body\"].read())['Source']\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    glueContext = GlueContext(sc)\n",
    "    job = Job(glueContext)\n",
    "    job.init(args['JOB_NAME'], args)\n",
    "    spark = glueContext.spark_session\n",
    "    for sourceinfo in config:\n",
    "        csvtoparquet(spark,config[sourceinfo],args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'malaysiaBefore': {'landing_prefix': 's3://{landingbucket}/malaysiaBefore',\n",
       "  'staging_prefix': 's3://{stagingbucket}/malaysia-before',\n",
       "  'cast': {'VendorId': 'int', 'PostalCode': 'int', 'VendorCode': 'int'},\n",
       "  'formatOptions': {'header': 'true',\n",
       "   'sep': ',',\n",
       "   'quote': '\"',\n",
       "   'escape': '\"',\n",
       "   'inferschema': 'true',\n",
       "   'nullValue': 'NULL'},\n",
       "  'partition_count': 1},\n",
       " 'malaysiaAfter': {'landing_prefix': 's3://{landingbucket}/malaysiaAfter',\n",
       "  'staging_prefix': 's3://{stagingbucket}/malaysia-after',\n",
       "  'cast': {'VendorId': 'int', 'PostalCode': 'int', 'VendorCode': 'int'},\n",
       "  'formatOptions': {'header': 'true',\n",
       "   'sep': ',',\n",
       "   'quote': '\"',\n",
       "   'escape': '\"',\n",
       "   'inferschema': 'true',\n",
       "   'nullValue': 'NULL'},\n",
       "  'partition_count': 1}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           EmailDate|\n",
      "+--------------------+\n",
      "|Tue, 18 Aug 2020 ...|\n",
      "|Tue, 18 Aug 2020 ...|\n",
      "|Tue, 18 Aug 2020 ...|\n",
      "|Tue, 18 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Tue, 18 Aug 2020 ...|\n",
      "|Tue, 18 Aug 2020 ...|\n",
      "|Tue, 18 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "|Wed, 19 Aug 2020 ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "malaysiaAfterDF.withColumn(\"PostalCode\",col('PostalCode').cast(\"int\")).withColumn(\"VendorCode\",col('VendorCode').cast(\"int\")).withColumn(\"VendorId\",col('VendorId').cast(\"int\")).where(\"EmailDate is not null\").select('EmailDate').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
